#### Reformer

https://zhuanlan.zhihu.com/p/115741192

https://zhuanlan.zhihu.com/p/105123890

https://www.zhihu.com/question/349958732/answer/945349902

https://zhuanlan.zhihu.com/p/48612853

https://www.zhihu.com/question/347833432/answer/836971492

https://zhuanlan.zhihu.com/p/120858314

https://zhuanlan.zhihu.com/p/99737208

[一文看懂 Attention（本质原理+3大优点+5大类型）](https://zhuanlan.zhihu.com/p/91839581)

#### 训练

[如何优雅地训练大型模型？](https://zhuanlan.zhihu.com/p/110278004)

[GPU 显存不足怎么办？](https://zhuanlan.zhihu.com/p/100884995)

[Pytorch有什么节省显存的小技巧？](https://www.zhihu.com/question/274635237/answer/755102181)


#### Bert / XLnet / Albert

[NLP必读：十分钟读懂谷歌BERT模型](https://zhuanlan.zhihu.com/p/51413773)

[NLP的游戏规则从此改写？从word2vec, ELMo到BERT](https://zhuanlan.zhihu.com/p/47488095)

[从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史](https://zhuanlan.zhihu.com/p/49271699)

[超细节的BERT/Transformer知识点](https://zhuanlan.zhihu.com/p/132554155)

[XLNet:运行机制及和Bert的异同比较](https://zhuanlan.zhihu.com/p/70257427)

[效果惊人的GPT 2.0模型：它告诉了我们什么](https://zhuanlan.zhihu.com/p/56865533)

[如何看待瘦身成功版BERT——ALBERT](https://www.zhihu.com/question/347898375)

[一文看尽2019年NLP前沿突破量子位](https://zhuanlan.zhihu.com/p/101416113)

https://zhuanlan.zhihu.com/p/132554155

[为什么Bert的三个Embedding可以进行相加？](https://www.zhihu.com/question/374835153)

[如何评价 BERT 模型？](https://www.zhihu.com/question/298203515)

[Bert比之Word2Vec,有哪些进步呢？](https://www.zhihu.com/question/352468546/answer/870846960)

https://zhuanlan.zhihu.com/p/46833276

#### padding

[NLP 中的Mask全解](https://zhuanlan.zhihu.com/p/139595546)


#### Transformer

https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec

code

https://jalammar.github.io/illustrated-transformer/

结构介绍的很清晰

[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)


[transformer中的attention为什么scaled?](https://www.zhihu.com/question/339723385)

https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)

https://towardsdatascience.com/transformers-141e32e69591

https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html

[When Recurrent Models Don't Need to be Recurrent](https://bair.berkeley.edu/blog/2018/08/06/recurrent/)

https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html

https://towardsdatascience.com/memory-attention-sequences-37456d271992


https://zhuanlan.zhihu.com/p/48508221

https://zhuanlan.zhihu.com/p/47282410

http://jalammar.github.io/illustrated-transformer/

这三篇差不多，讲的都很好

https://zhuanlan.zhihu.com/p/38485843

挺好，可再看

https://zhuanlan.zhihu.com/p/47812375

https://www.zhihu.com/question/302392659/answer/551542493

https://www.zhihu.com/question/311377593/answer/591236505

https://zhuanlan.zhihu.com/p/48731949

https://www.zhihu.com/question/313507574/answer/645129386

https://zhuanlan.zhihu.com/p/52924578

https://zhuanlan.zhihu.com/p/59629215

https://zhuanlan.zhihu.com/p/39034683

https://zhuanlan.zhihu.com/p/54356280




https://zhuanlan.zhihu.com/p/54770086




https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html

https://medium.com/@adityathiruvengadam/transformer-architecture-attention-is-all-you-need-aeccd9f50d09

