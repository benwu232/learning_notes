#### Reformer

https://zhuanlan.zhihu.com/p/115741192

https://zhuanlan.zhihu.com/p/105123890

https://www.zhihu.com/question/349958732/answer/945349902

https://zhuanlan.zhihu.com/p/48612853

https://www.zhihu.com/question/347833432/answer/836971492

https://zhuanlan.zhihu.com/p/120858314

https://zhuanlan.zhihu.com/p/99737208

[一文看懂 Attention（本质原理+3大优点+5大类型）](https://zhuanlan.zhihu.com/p/91839581)

#### 训练

[如何优雅地训练大型模型？](https://zhuanlan.zhihu.com/p/110278004)

[GPU 显存不足怎么办？](https://zhuanlan.zhihu.com/p/100884995)

[Pytorch有什么节省显存的小技巧？](https://www.zhihu.com/question/274635237/answer/755102181)


#### Bert / XLnet / Albert

[NLP必读：十分钟读懂谷歌BERT模型](https://zhuanlan.zhihu.com/p/51413773)

[NLP的游戏规则从此改写？从word2vec, ELMo到BERT](https://zhuanlan.zhihu.com/p/47488095)

[从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史](https://zhuanlan.zhihu.com/p/49271699)

[超细节的BERT/Transformer知识点](https://zhuanlan.zhihu.com/p/132554155)

[XLNet:运行机制及和Bert的异同比较](https://zhuanlan.zhihu.com/p/70257427)

[效果惊人的GPT 2.0模型：它告诉了我们什么](https://zhuanlan.zhihu.com/p/56865533)

[如何看待瘦身成功版BERT——ALBERT](https://www.zhihu.com/question/347898375)

[一文看尽2019年NLP前沿突破量子位](https://zhuanlan.zhihu.com/p/101416113)

https://zhuanlan.zhihu.com/p/132554155

[为什么Bert的三个Embedding可以进行相加？](https://www.zhihu.com/question/374835153)

[如何评价 BERT 模型？](https://www.zhihu.com/question/298203515)

[Bert比之Word2Vec,有哪些进步呢？](https://www.zhihu.com/question/352468546/answer/870846960)

https://zhuanlan.zhihu.com/p/46833276

[【NLP】Universal Transformers详解](https://zhuanlan.zhihu.com/p/44655133)

[谷歌的机器翻译模型 Transformer，现在可以用来做任何事了](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650747087&idx=3&sn=0157141d97fa2f7f8907e840d68def5d&chksm=871af4b1b06d7da7a18fe60375be52a7b40739f1972efba50b7712c67c7cfd93f00aad4dced0&mpshare=1&scene=24&srcid=0817VCBRh4QR9EkZV9R1W2xg#rd)

[性能媲美BERT却只有其1/10参数量？ | 近期最火模型ELECTRA解析](https://mp.weixin.qq.com/s?__biz=MzAxMDk0OTI3Ng==&mid=2247484022&idx=1&sn=efa203443f39e055c8d575c4f1ad6c33&chksm=9b49c585ac3e4c9388bbd9f40b57cf14344f76d5e30a8876ccc66c4da5a2bdad09e1c81fc2a6&scene=21#wechat_redirect)

#### padding & mask

[NLP 中的Mask全解](https://zhuanlan.zhihu.com/p/139595546)

[深度学习中的mask到底是什么意思](https://www.zhihu.com/question/320615749/answer/1080485410)

[为什么RNN 需要mask 输入](https://www.zhihu.com/question/55014250/answer/214236204)

[从语言模型到Seq2Seq：Transformer如戏，全靠Mask](https://zhuanlan.zhihu.com/p/84157931)

#### positional encoding

[如何理解Transformer论文中的positional encoding](https://www.zhihu.com/question/347678607)

[BERT为何使用学习的position embedding而非正弦position encoding?](https://www.zhihu.com/question/307293465)

https://kazemnejad.com/blog/transformer_architecture_positional_encoding/

#### Transformer

https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec

code

https://jalammar.github.io/illustrated-transformer/

结构介绍的很清晰

[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)

[Transformer注解及PyTorch实现](https://www.cnblogs.com/guoyaohua/p/transformer.html)

[聊聊 Transformer](https://zhuanlan.zhihu.com/p/47812375)

[Transformer 模型的 PyTorch 实现](https://juejin.im/post/5b9f1af0e51d450e425eb32d#comment)

[Transformer模型的PyTorch实现](https://luozhouyang.github.io/transformer/) 解释了一些背后的原理

http://www.360doc.com/content/20/0115/13/7673502_886318289.shtml



[详解Transformer](https://zhuanlan.zhihu.com/p/48508221)

[transformer中的attention为什么scaled?](https://www.zhihu.com/question/339723385)

https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)

https://towardsdatascience.com/transformers-141e32e69591

https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html

[When Recurrent Models Don't Need to be Recurrent](https://bair.berkeley.edu/blog/2018/08/06/recurrent/)

https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html

https://towardsdatascience.com/memory-attention-sequences-37456d271992


https://zhuanlan.zhihu.com/p/48508221

https://zhuanlan.zhihu.com/p/47282410

http://jalammar.github.io/illustrated-transformer/

这三篇差不多，讲的都很好

https://zhuanlan.zhihu.com/p/38485843

挺好，可再看

https://zhuanlan.zhihu.com/p/47812375

https://www.zhihu.com/question/302392659/answer/551542493

https://www.zhihu.com/question/311377593/answer/591236505

https://zhuanlan.zhihu.com/p/48731949

https://www.zhihu.com/question/313507574/answer/645129386

https://zhuanlan.zhihu.com/p/52924578

https://zhuanlan.zhihu.com/p/59629215

https://zhuanlan.zhihu.com/p/39034683

https://zhuanlan.zhihu.com/p/54356280




https://zhuanlan.zhihu.com/p/54770086




https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html

https://medium.com/@adityathiruvengadam/transformer-architecture-attention-is-all-you-need-aeccd9f50d09

### trend

- [ICLR 2020趋势分析：NLP中更好&更快的Transformer](https://zhuanlan.zhihu.com/p/143852458)
- https://zhuanlan.zhihu.com/p/141624736
- [从BERT、XLNet到MPNet，细看NLP预训练模型发展变迁史](https://zhuanlan.zhihu.com/p/146325984)
- [2020年NLP所有领域最新、经典、顶会、必读论文整理分享](https://zhuanlan.zhihu.com/p/143123368)
- [transformer三部曲](https://zhuanlan.zhihu.com/p/85612521)
- [ICML(2020)自然语言处理（NLP）论文汇总](https://zhuanlan.zhihu.com/p/145798732)



