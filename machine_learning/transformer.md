#### Reformer

https://zhuanlan.zhihu.com/p/115741192

https://zhuanlan.zhihu.com/p/105123890

https://www.zhihu.com/question/349958732/answer/945349902

https://zhuanlan.zhihu.com/p/48612853

https://www.zhihu.com/question/347833432/answer/836971492

https://zhuanlan.zhihu.com/p/120858314

https://zhuanlan.zhihu.com/p/99737208

[GPU 显存不足怎么办？](https://zhuanlan.zhihu.com/p/100884995)

[一文看懂 Attention（本质原理+3大优点+5大类型）](https://zhuanlan.zhihu.com/p/91839581)


#### Transformer

https://jalammar.github.io/illustrated-transformer/

结构介绍的很清晰

https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)

https://towardsdatascience.com/transformers-141e32e69591

https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html

[When Recurrent Models Don't Need to be Recurrent](https://bair.berkeley.edu/blog/2018/08/06/recurrent/)

https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html

https://towardsdatascience.com/memory-attention-sequences-37456d271992


https://zhuanlan.zhihu.com/p/48508221

https://zhuanlan.zhihu.com/p/47282410

http://jalammar.github.io/illustrated-transformer/

这三篇差不多，讲的都很好

https://zhuanlan.zhihu.com/p/38485843

挺好，可再看

https://zhuanlan.zhihu.com/p/47812375

https://www.zhihu.com/question/302392659/answer/551542493

https://www.zhihu.com/question/311377593/answer/591236505

https://zhuanlan.zhihu.com/p/48731949

https://www.zhihu.com/question/313507574/answer/645129386

https://zhuanlan.zhihu.com/p/52924578

https://zhuanlan.zhihu.com/p/59629215

https://zhuanlan.zhihu.com/p/39034683

https://zhuanlan.zhihu.com/p/54356280




https://zhuanlan.zhihu.com/p/54770086




https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html

https://medium.com/@adityathiruvengadam/transformer-architecture-attention-is-all-you-need-aeccd9f50d09

